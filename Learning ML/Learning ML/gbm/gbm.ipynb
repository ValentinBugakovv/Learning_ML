{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Построение композиции — важный подход в машинном обучении, который позволяет<br> объединять большое количество слабых алгоритмов в один сильный.<br> Данный подход широко используется на практике в самых разных задачах.</p>\n",
    "<p>На лекциях был рассмотрен метод градиентного бустинга, который последовательно<br> строит композицию алгоритмов, причем каждый следующий алгоритм выбирается так,<br> чтобы исправлять ошибки уже имеющейся композиции. Обычно в качестве базовых<br> алгоритмов используют деревья небольшой глубины, поскольку их достаточно легко<br> строить, и при этом они дают нелинейные разделяющие поверхности.</p>\n",
    "<p>Другой метод построения композиций — случайный лес. В нем, в отличие от<br> градиентного бустинга, отдельные деревья строятся независимо и без каких-либо<br> ограничений на глубину — дерево наращивается до тех пор, пока не покажет наилучшее качество на обучающей выборке.</p>\n",
    "<p>В этом задании мы будем иметь дело с задачей классификации. В качестве функции потерь будем использовать log-loss</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите выборку из файла gbm-data.csv с помощью pandas и преобразуйте ее в массив numpy (параметр values у\n",
    "датафрейма). В первой колонке файла с данными записано, была или нет реакция. Все остальные колонки (d1 - d1776)\n",
    "содержат различные характеристики молекулы, такие как размер, форма и т.д. Разбейте выборку на обучающую и тестовую,\n",
    "используя функцию train_test_split с параметрами test_size = 0.8 и random_state = 241.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('gbm-data.csv')\n",
    "y = df['Activity'].values\n",
    "X = df.loc[:, 'D1':'D1776'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=241)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите GradientBoostingClassifier с параметрами n_estimators=250, verbose=True, random_state=241 и для каждого\n",
    "значения learning_rate из списка [1, 0.5, 0.3, 0.2, 0.1] проделайте следующее:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(y_pred):\n",
    "    # Преобразуйте полученное предсказание с помощью сигмоидной функции по формуле 1 / (1 + e^{−y_pred}),\n",
    "    # где y_pred — предсказаное значение.\n",
    "    return 1.0 / (1.0 + math.exp(-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss_results(model, X, y):\n",
    "    # Используйте метод staged_decision_function для предсказания качества на обучающей и тестовой выборке\n",
    "    # на каждой итерации.\n",
    "    results = []\n",
    "    for pred in model.staged_decision_function(X):\n",
    "        results.append(log_loss(y, [sigmoid(y_pred) for y_pred in pred]))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(learning_rate, test_loss, train_loss):\n",
    "    # Вычислите и постройте график значений log-loss (которую можно посчитать с помощью функции\n",
    "    # sklearn.metrics.log_loss) на обучающей и тестовой выборках, а также найдите минимальное значение метрики и\n",
    "    # номер итерации, на которой оно достигается.\n",
    "#     plt.figure()\n",
    "#     plt.plot(test_loss, 'r', linewidth=2)\n",
    "#     plt.plot(train_loss, 'g', linewidth=2)\n",
    "#     plt.legend(['test', 'train'])\n",
    "#     plt.savefig('plots/rate_' + str(learning_rate) + '.png')\n",
    "\n",
    "    min_loss_value = min(test_loss)\n",
    "    min_loss_index = test_loss.index(min_loss_value)\n",
    "    return min_loss_value, min_loss_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(learning_rate):\n",
    "    model = GradientBoostingClassifier(learning_rate=learning_rate, n_estimators=250, verbose=True, random_state=241)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_loss = log_loss_results(model, X_train, y_train)\n",
    "    test_loss = log_loss_results(model, X_test, y_test)\n",
    "    return plot_loss(learning_rate, test_loss, train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.0190           18.17s\n",
      "         2           0.9192           20.58s\n",
      "         3           0.8272           17.70s\n",
      "         4           0.7834           16.42s\n",
      "         5           0.7109           15.43s\n",
      "         6           0.6368           15.00s\n",
      "         7           0.5797           14.51s\n",
      "         8           0.5610           14.15s\n",
      "         9           0.5185           13.81s\n",
      "        10           0.4984           13.41s\n",
      "        20           0.1999           12.37s\n",
      "        30           0.1313           11.91s\n",
      "        40           0.0790           11.42s\n",
      "        50           0.0511           10.68s\n",
      "        60           0.0352            9.97s\n",
      "        70           0.0245            9.39s\n",
      "        80           0.0162            8.80s\n",
      "        90           0.0114            8.31s\n",
      "       100           0.0077            7.73s\n",
      "       200           0.0004            2.23s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1255           13.69s\n",
      "         2           1.0035           13.76s\n",
      "         3           0.9386           13.58s\n",
      "         4           0.8844           13.22s\n",
      "         5           0.8381           13.13s\n",
      "         6           0.7995           12.73s\n",
      "         7           0.7559           12.46s\n",
      "         8           0.7205           12.31s\n",
      "         9           0.6958           12.32s\n",
      "        10           0.6725           12.17s\n",
      "        20           0.4672           11.80s\n",
      "        30           0.3179           11.52s\n",
      "        40           0.2274           11.20s\n",
      "        50           0.1774           10.65s\n",
      "        60           0.1394           10.18s\n",
      "        70           0.1050            9.68s\n",
      "        80           0.0805            9.03s\n",
      "        90           0.0650            8.40s\n",
      "       100           0.0511            7.97s\n",
      "       200           0.0058            2.56s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2095           13.94s\n",
      "         2           1.1006           13.89s\n",
      "         3           1.0240           13.66s\n",
      "         4           0.9729           13.77s\n",
      "         5           0.9387           13.23s\n",
      "         6           0.8948           13.17s\n",
      "         7           0.8621           12.95s\n",
      "         8           0.8360           12.70s\n",
      "         9           0.8171           12.72s\n",
      "        10           0.7883           12.55s\n",
      "        20           0.6164           11.41s\n",
      "        30           0.4933           10.69s\n",
      "        40           0.4248           10.26s\n",
      "        50           0.3345            9.76s\n",
      "        60           0.2760            9.23s\n",
      "        70           0.2263            8.74s\n",
      "        80           0.1971            8.25s\n",
      "        90           0.1693            7.74s\n",
      "       100           0.1388            7.27s\n",
      "       200           0.0294            2.46s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2613           13.44s\n",
      "         2           1.1715           13.51s\n",
      "         3           1.1009           13.50s\n",
      "         4           1.0529           13.96s\n",
      "         5           1.0130           14.94s\n",
      "         6           0.9740           14.68s\n",
      "         7           0.9475           14.09s\n",
      "         8           0.9197           14.06s\n",
      "         9           0.8979           13.76s\n",
      "        10           0.8730           13.61s\n",
      "        20           0.7207           11.96s\n",
      "        30           0.6055           11.17s\n",
      "        40           0.5244           10.52s\n",
      "        50           0.4501            9.94s\n",
      "        60           0.3908            9.39s\n",
      "        70           0.3372            8.88s\n",
      "        80           0.3009            8.37s\n",
      "        90           0.2603            7.88s\n",
      "       100           0.2327            7.36s\n",
      "       200           0.0835            2.51s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3199           13.94s\n",
      "         2           1.2645           14.01s\n",
      "         3           1.2170           14.41s\n",
      "         4           1.1775           14.94s\n",
      "         5           1.1404           16.41s\n",
      "         6           1.1106           15.98s\n",
      "         7           1.0844           15.62s\n",
      "         8           1.0617           15.61s\n",
      "         9           1.0411           15.37s\n",
      "        10           1.0223           15.16s\n",
      "        20           0.8864           13.74s\n",
      "        30           0.7844           12.53s\n",
      "        40           0.7176           11.85s\n",
      "        50           0.6590           11.41s\n",
      "        60           0.6120           10.81s\n",
      "        70           0.5599           10.39s\n",
      "        80           0.5242            9.64s\n",
      "        90           0.4829            9.09s\n",
      "       100           0.4473            8.40s\n",
      "       200           0.2379            2.73s\n"
     ]
    }
   ],
   "source": [
    "min_loss_results = {}\n",
    "for learning_rate in [1, 0.5, 0.3, 0.2, 0.1]:\n",
    "    min_loss_results[learning_rate] = model_test(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно охарактеризовать график качества на тестовой выборке, начиная с некоторой итерации:переобучение\n",
    "(overfitting) или недообучение (underfitting)? В ответе укажите одно из слов overfitting либо underfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overfitting\n"
     ]
    }
   ],
   "source": [
    "print('overfitting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "приведите минимальное значение log-loss и номер итерации, на котором оно достигается, при learning_rate = 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.1: (0.5269201872275844, 51),\n",
       " 0.2: (0.531094637596885, 36),\n",
       " 0.3: (0.5423141110024554, 10),\n",
       " 0.5: (0.5582025523164261, 6),\n",
       " 1: (0.5822942594278476, 0)}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_loss_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53 36\n"
     ]
    }
   ],
   "source": [
    "min_loss_value, min_loss_index = min_loss_results[0.2]\n",
    "print('{:0.2f} {}'.format(min_loss_value, min_loss_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этих же данных обучите RandomForestClassifier с количеством деревьев, равным количеству итераций, на котором\n",
    "достигается наилучшее качество у градиентного бустинга из предыдущего пункта, c random_state=241 и остальными\n",
    "параметрами по умолчанию. Какое значение log-loss на тесте получается у этого случайного леса? (Не забывайте, что\n",
    "предсказания нужно получать с помощью функции predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5413812861804069\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=min_loss_index, random_state=241)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict_proba(X_test)[:, 1]\n",
    "test_loss = log_loss(y_test, y_pred)\n",
    "print(test_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
